% !TEX root = main.tex

\www{
%After having outlined the preliminaries and the mining model in Sections~\ref{sec:preliminaries} and \ref{sec:pca},
We now outline the core algorithm of AMIE and its implementation.
%Again, we follow the explications in \cite{amie}.
We follow the explications in \cite{amie} and extend them with further explanations and details.
}

\www{
%- - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
\subsection{Algorithm}
\label{subsec:algorithm}
\comment{R2}{Similarly, the Algorithm 2 is not very accurate. It would be clearer if "distinct" is added on line 15: "for all distinct x $\in$ SELECT ?x FROM K WHERE q do." (In contrast, the DISTINCT in the query on the top of Page 11 is redundant).}

\comment{R3}{Here are some of my criticisms, which apply to Section 5 on AMIE - your previous work:
1.Algorithm 1 is too high level and as such it leaves too much unexpressed (e.g. "execute in parallel", "is not pruned for output").
2.      SQL and SPARQL are presented at page 11 only to say that they are discarded for a custom implementation.
3.      The "vanilla in-memory database" implementation is too informally described (e.g.: each "index is a map from the first item to a map from the second item to a set of the third item": a more formal description and a figure, please!
}

\paragraph{Goal} Our goal is to mine rules of the form defined in Section~\ref{sec:preliminaries}.
One of the main problems of any mining approach is to find an efficient way to explore the search space. The naive algorithm of enumerating all possible rules is infeasible for large KBs.
Hence, we explore the search space by iteratively extending rules by \emph{mining operators}.
}

\www{
\paragraph{Mining Operators}
We see a rule as a sequence of atoms. The first atom is the head atom and the others are the body atoms. In the process of traversing the search space, we can extend a rule by using one of the following operators:
\begin{enumerate}
\item \textbf{Add Dangling Atom ($\mathcal{O}_D$})\\
This operator adds a new atom to a rule. The new atom uses a fresh variable for one of its two arguments. The other argument is a variable
that is shared with the rule, i.e., it occurs in some other atom of the rule.
\item \textbf{Add Instantiated Atom ($\mathcal{O}_I$})\\
This operator adds a new atom to a rule that uses an entity for one argument and shares the other argument (variable or entity) with the rule.
\item \textbf{Add Closing Atom ($\mathcal{O}_C$})\\
This operator adds a new atom to a rule so that both of its arguments are shared with the rule.
\end{enumerate}
By repeated application of these operators, we can generate the entire space of rules as defined in Section~\ref{sec:preliminaries}.
The operators generate even more rules than those that we are interested in, because they also produce rules that are not closed.
An alternative set of operators could consist of $\mathcal{O}_D$ and an operator for instantiation.
But these operators would not be monotonic, in the sense that an atom generated by one operator can be modified in the next step by the other operator.
Therefore, we chose the above 3 operators as a canonic set.
}


\paragraph{Algorithm} \label{algo}
Algorithm~\ref{rm} sketches our approach to mine rules. The algorithm maintains a queue of rules, 
which initially contains all possible head atoms, that is,
all rules of size 1.
The algorithm iteratively dequeues a rule from the queue. If the rule 
meets certain criteria (Lines 6 and 7), then it is output.
% \comment{Chris}{its quality is evaluated in terms of (PCA) confidence and if it passes the corresponding threshold} \comment{Luis: }{We threshold on PCA confidence
%  only for AMIE+}\comment{Chris}{so, are we going to output a rule with confidence 0\%?},
Then, the algorithm applies all mining operators to the rule. This results in more rules which are added to 
the queue if they pass the head coverage threshold $\theta$ (Lines 9 to 15).
This process is repeated until the queue is empty. 
To speed up the process, our implementation parallelizes Algorithm~\ref{rm}, that is, the main loop (Lines 4 to 16) runs
in multiple threads. 
This achieved by synchronizing the access to the centralized queue from which the threads dequeue and enqueue.
We do not feed predictions of the rules back into the KB. All measures (such as confidence and support) are always computed on the original KB.


% \comment{Chris}{about the figure: shouldn't there be a line stating that the rule is evaluated somehow?}
% Fabian: We discussed this and agreed to leave it as it is until the need arises.

\begin{algorithm}
\caption{Rule Mining}
\label{rm}
\begin{algorithmic}[1]
\Function{AMIE}{KB $\mathcal{K}$, $\theta$, $minConf$}
    \State $q = \langle [r_1(x,y), r_2(x,y) \dots r_m(x,y)] \rangle$
    \State $out = \langle \rangle$
	\While{$\neg q$\emph{.isEmpty}()}
	  \State $r = q.$\emph{dequeue}()
	  \If{$AcceptedForOutput(r, out, minConf)$}
	    \If{$r \notin out$}
	      \State $out.$\emph{add}$(r)$
	    \EndIf
	  \EndIf
	  \ForAll{operators $o$}
	    \ForAll{rules $r' \in o(r)$}
		  \If{$hc(r') \ge \theta$}
		    \If{$r' \notin q$}
		      \State $q.$\emph{enqueue}$(r')$
		    \EndIf
		  \EndIf
		\EndFor
	  \EndFor
	\EndWhile
    \State \Return $out$
\EndFunction
\end{algorithmic}
\end{algorithm}

\begin{algorithm}
\caption{Routine to decide to output a rule}
\label{pfo}
\begin{algorithmic}[1]
\Function{AcceptedForOutput}{rule $r$, $out$, $minConf$}
    \If{$r$ is not closed $\vee\; pcaconf(r) < minConf$}
      \State \Return $false$
    \EndIf 
    \State $parents = parentsOfRule(r, out)$
    \ForAll{$r_p \in parents$}
      \If{$pcaConf(r) < pcaconf(r_p)$}
	\State \Return $false$
      \EndIf
    \EndFor
    \State \Return $true$
\EndFunction
\end{algorithmic}
\end{algorithm}

% \www{
% \paragraph{Pruning} If executed naively, our algorithm will have prohibitively high runtimes. The instantiation operator $\mathcal{O}_I$, in particular, generates atoms in the order of $|\mathcal{R}| \times |\mathcal{E}|$. We first observe that we are usually not interested in rules that cover only very few facts of the head relation.
% % \comment{Katja}{Would a proper definition of what ''instances`` means in this context be useful?}
% % Fabian: switched to "facts of head relation"
% Rules that cover, for example, less than 1\% of the facts of the head relation can safely assumed to be marginal. Therefore, we set $\theta=0.01$ as a lower bound for the head coverage. We observe that head coverage decreases monotonically as we add more atoms. This allows us to safely discard any rule that trespasses the threshold (Lines 11 and 12).
% % \comment{Katja}{Maybe add the threshold itself to the pseudocode algorithm.}
% % Fabian: Yes, this is an option. I wanted to keep the algorithm general, and outsource the definition of "pruning" to this paragraph here. This is because the algorithm is first discussed in generality in the previous section.
% }


%\comment{Katja}{How about some restructuring (introducing subsections) that shows more clearly what paragraph belongs where, i.e., at the moment ''Algorithm``, ''Pruning``, ''Reducing the output size``, etc. are all on the same level (all paragraphs).}

\subsection{Stages of Rule Mining}

\subsubsection{Output a Rule}  
\label{subsubsec:whenToOutput}
Algorithm~\ref{pfo} describes the routine to decide if a rule should be output or 
not once it has been dequeued. If the rule is not closed (See Section~\ref{subsec:rules}) or 
its PCA confidence is below the given confidence threshold, the algorithm discards it for output. 
Recall that AMIE is conceived to mine rules that can predict concrete facts and therefore, thus non-closed rules are
seen as intermediate steps. Moreover, the PCA confidence is only defined for closed rules.
It is important to remark, that the confidence threshold is enforced 
very late in the rule mining process, i.e., right before reporting the rule. 
This occurs because confidence is not monotonic, that is, the addition of atoms to a rule
can both increase or decrease its confidence. This means that confidence is not suitable for accurate
pruning. Enforcing the confidence threshold at the end also implies that
the runtime of AMIE is not affected by the magnitude of this value. For this reason, 
the confidence threshold is an optional argument for AMIE. If omitted, the system assumes a value of 0. 

If the rule is closed and confident enough, Algorithm~\ref{pfo} then applies a \emph{skyline technique} (Lines 5 to 10)
to reduce the output. If a rule $B_1 \wedge ... \wedge B_n \wedge B_{n+1} \Rightarrow H$ does not have larger confidence
than its parent rule $B_1 \wedge ... \wedge B_n \Rightarrow H$, then we do not output the longer rule.
Since support and head coverage are monotonic metrics, we know that the child rule will never have a higher value than its parent rule. 
If the child rule has lower confidence, then its quality is worse in all aspects and there is 
no reason to include it in the output.
In addition, notice that a rule can have multiple parents, for instance, the rule $actedIn(x,y) \wedge directedIn(x,y) \Rightarrow created(x,y)$
can be derived by applying a closing-atom operator $\mathcal{O}_C$ to either $actedIn(x,y) \Rightarrow created(x,y)$ or
$directedIn(x,y) \Rightarrow created(x,y)$. While in practice the longer rule is derived from one of those rules, AMIE
finds all its potential parents (Line 5) and applies the skyline technique for each of them (Lines 6 to 10), i.e., the child
rule must have higher confidence than all its parents to be accepted.
We emphasize that the skyline technique is only applied to prune the output. 
Since confidence can increase with the addition of atoms, we should still use the longer rule 
for further refinement.

% We never enqueue a rule that is already in the queue. It is expensive to check two rules for equality.
% However, it is easy to compute the support for each rule. Two rules can only be equal if they have the same head relation and
% the same support. This reduces the set of rules that have to be checked. If a rule is duplicate, we do not enqueue it (lines 11 and 12 in Algorithm~\ref{rm}).
% 
% Since our algorithm performs a breadth-first-search, we can be sure that for any new rule, its potential duplicates will still be in the queue.
% When we dequeue a rule with $n$ atoms, no rule with $n+1$ atoms has ever been dequeued. Thus, when we apply the mining operators to the rule with $n$ atoms, and generate a rule with $n+1$ atoms,
% any potential duplicate of that new rule must be in the queue.


\subsubsection{Pruning} 
\label{subsubsec:pruning}
\www{
If executed naively, Algorithm~\ref{rm} will have prohibitively high runtimes.
The instantiation operator $\mathcal{O}_I$, in particular, generates atoms in the order of $|\mathcal{R}| \times |\mathcal{E}|$.
We first observe that we are usually not interested in rules that cover only very few facts of the head relation.
Rules that cover, for example, less than 1\% of the facts of the head relation can safely be assumed to be marginal.
Therefore, we choose $\theta=0.01$ as a lower bound for the head coverage. We observe that head coverage decreases monotonically as we add more atoms.
This allows for safely discarding any rule that trespasses the threshold (lines 11 and 12 in Algorithm~\ref{rm}).
}
Recall from Section~\ref{subsec:statSignificance} that support and head coverage are defined even for rules that are not yet closed, allowing for early pruning.

\subsubsection{Duplicate elimination} 
\label{subsubsec:duplicateElimination}
As mentioned in Section~\ref{subsubsec:whenToOutput} a rule can be derived in multiple ways.
For example, the rule $actedIn(x,y) \wedge directedIn(x,y) \Rightarrow created(x,y)$ can result from the application
of the operator $\mathcal{O}_C$ to both $actedIn(x,y) \Rightarrow created(x,y)$ and $directedIn(x,y) \Rightarrow created(x,y)$.


We never enqueue a rule that is already in the queue. It is expensive to check two rules for equality.
However, it is easy to compute the support for each rule. Two rules can only be equal if they have the same head relation and
the same support. This reduces the set of rules that have to be checked. If a rule is duplicate, we do not enqueue it (lines 11 and 12 in Algorithm~\ref{rm}).

Since our algorithm performs a breadth-first-search, we can be sure that for any new rule, its potential duplicates will still be in the queue.
When we dequeue a rule with $n$ atoms, no rule with $n+1$ atoms has ever been dequeued. Thus, when we apply the mining operators to the rule with $n$ atoms, and generate a rule with $n+1$ atoms,
any potential duplicate of that new rule must be in the queue.

% \paragraph{Reducing the Output Size}  We can reduce the number of output rules in many different ways.
% If a rule $B_1 \wedge ... \wedge B_n \wedge B_{n+1} \Rightarrow H$ does not have larger confidence
% than the rule $B_1 \wedge ... \wedge B_n \Rightarrow H$, then we do not output the longer rule (lines 6 and 7 in Algorithm~\ref{rm}).
% Since head coverage is monotonic, we know that the longer rule will have smaller head coverage than its parent rule, and we also do not expect it to be of higher quality because its confidence is lower. %behave better quality-wise.
% Therefore, there is no reason to include the longer rule in the output.
% However, since confidence is not monotonic, we can possibly obtain rules of higher confidence by adding even more atoms.
% Therefore, we still use the longer rule for further refinement.
% 
% We never enqueue a rule that is already in the queue. It is expensive to check two rules for equality.
% However, it is easy to compute the support for each rule. Two rules can only be equal if they have the same head relation and
% the same support. This reduces the set of rules that have to be checked. If a rule is duplicate, we do not enqueue it (lines 11 and 12 in Algorithm~\ref{rm}).
% 
% Since our algorithm performs a breadth-first-search, we can be sure that for any new rule, its potential duplicates will still be in the queue.
% When we dequeue a rule with $n$ atoms, no rule with $n+1$ atoms has ever been dequeued. Thus, when we apply the mining operators to the rule with $n$ atoms, and generate a rule with $n+1$ atoms,
% any potential duplicate of that new rule must be in the queue.




% \www{
% The monotonicity of head coverage gives us another opportunity to prune: If a rule $B_1 \wedge ... \wedge B_n \wedge B_{n+1} \Rightarrow H$ does not have larger confidence
% than the rule $B_1 \wedge ... \wedge B_n \Rightarrow H$, then we do not output the longer rule.
% This is because both the confidence and the head coverage of the longer rule are necessarily dominated by the shorter rule.
% This way, we can reduce the number of produced rules (Lines 6 and 7).
% }
%
% \www{
% Last, we never enqueue a rule that is already in the queue. It is expensive to check two rules for equality.
% However, it is easy to compute measures such as head coverage, confidence, and PCA confidence for each rule. Two rules can only be equal if they have the same values for these measures.
% This restricts the rules that have to be checked. If a rule is duplicate, we do not enqueue it (Lines 11 and 12).
% We can be sure that any potential duplicates will still be in the queue. This is because the length of the rules increases monotonically: When we dequeue a rule with $n$ atoms, no rule with $n+1$ atoms has ever been dequeued. Thus, when we apply the operators to the rule with $n$ atoms, and generate a rule with $n+1$ atoms, any potential duplicate of that new rule must be in the queue.
% }

\ignore{
% Fabian: Problem solved. Thanks, Luis!
\comment{Fabian}{Problem: what if the duplicate rule has already been dequeued by some other thread?}
\comment{Katja}{Use a pointer over the queue that points to the next item to be dequeued by the next thread. The first positions correspond to rules that are currently considered by active threads. Remove the rule when the thread is finished. Check all rules for duplicates when trying to add a new rule to the queue.}
\comment{Luis}{We use a set that guarantees insertion order at iteration time (LinkedHashSet) to explore the space in a breath search fashion. That means that by the time the first of size n is dequeued, all rules of size n are already in the
queue and duplicates have been already pruned.}
}

% \www{
% \paragraph{Projection Queries}
% No matter what operator is applied in particular, the algorithm needs to choose a relation for the new atom that is added to the rule. In addition,
% the instantiation operator $\mathcal{O_I}$ also allows the choice of an entity. In order to select only relations and entities that will fulfill the head coverage constraint, we rely on the KB to answer \emph{projection queries}. These are queries of the form
% \indented{SELECT $?x$ WHERE $H \wedge B_1 \wedge ... \wedge B_n$\\
% HAVING COUNT($H$)$\geq k$}
% where $B_1, ..., B_n$ are atoms and $k$ is a natural number. $H$ is the \emph{projection atom} on which we project. $?x$ is the \emph{selection variable}. It is a variable that appears in one or more atoms at the position of one of the arguments or at the position of the relation (as it is common in SPARQL\footnote{\url{http://www.w3.org/TR/rdf-sparql-query/}}). Such queries select an entity or relation $x$ such that the result of the query $H \wedge B_1 \wedge ... \wedge B_n$ on the KB contains more than $k$ distinct query answers for $H$.
% }

%\comment{Katja}{Some hierarchy of paragraphs/subsections would probably also be useful in the following.}
\subsection{Count-Projection Queries}
\label{projectionQueries}

%\paragraph{Projection Queries}
No matter what operator is applied in particular, the algorithm needs to choose a relation for the new atom that will be added to the rule.
In addition, the instantiation operator $\mathcal{O_I}$ also allows the choice of an entity.
In order to select only relations and entities that will fulfill the head coverage constraint, we rely on the KB to answer \emph{count-projection queries}.
These are queries of the form
\indented{SELECT $?x$, COUNT($H$) \\
WHERE $H \wedge B_1 \wedge ... \wedge B_n$\\
SUCH THAT COUNT($H$)$\geq k$}
%\comment{Katja}{It still looks weird to have a HAVING clause without a GROUP BY.}
where $B_1, ..., B_n$ are atoms and $k$ is a natural number.
% \comment{Katja}{Strictly speaking we project on $?x$ as this is the only value in the select clause. It would be safe to say that we project on $?x$ but select (have an additional condition) on $H$. So, $H$ could be the ``group atom''.}
% \comment{Luis :}{I agree with Katja. We should be strict in notation. I wonder whether we should stop calling them ``projection queries''
% as it sounds too generic.}
$?x$ is the \emph{selection variable}.
It is a variable that appears in one or more atoms at the position of one of the arguments or at the position of the relation (as it is common in SPARQL~\cite{sparql}). 
Such queries select an entity or relation $x$ such that the result of the query $H \wedge B_1 \wedge ... \wedge B_n$ on the KB contains more than $k$ distinct query answers
for the \emph{group atom} $H$. The group atom corresponds to the head of the rule.
% Fabian: we never said it, so the reader cannot recall
%Recall that the expression COUNT($H$) binds to the support
%of the whole query pattern for each value of the selection variable $?x$.

We will discuss the implementation of such queries and their relation to SPARQL and SQL queries below. For now, we will use the above pseudo-syntax.

\paragraph{Using Projection Queries} Projection queries allow us to select the relationship for the operators $\mathcal{O_D}$, $\mathcal{O_I}$, and $\mathcal{O_C}$ in such a way
that the head coverage of the resulting rule is above $\theta$.
 This works by firing a projection query of the form
\\ \\
SELECT $?r$, COUNT($H$)\\
WHERE $H \wedge B_1 \wedge ... \wedge B_{n-1}\; \wedge\; ?r(X,Y)$\\
SUCH THAT COUNT($H$)$\geq k$
\\ \\
%\comment{Katja}{HAVING without GROUP BY, see above}
where $k := \theta \times size(H)$. $X$ and $Y$ are variables or constants, depending on the type of atoms that the operator generates.
The results for $?r$ are the relations that, once bound in the query, ensure that the head coverage of the rule $B_1 \; \wedge ... \wedge B_{n-1} \;\wedge\; ?r(X,Y) \Rightarrow H$ is greater than $\theta$.
For instance, assume
Algorithm~\ref{rm} dequeues the following intermediate non-closed rule for further refinement.

\begin{center}
\emph{isMarriedTo}$(x,z) \Rightarrow $ \emph{livesIn}$(x,y)$
\end{center}

\noindent
The application of the operator $\mathcal{O_D}$ will fire queries of the form:\\ \\
SELECT $?r$, COUNT($livesIn(x,y)$)\\
WHERE $livesIn(x,y) \; \wedge \; isMarriedTo(x,z) \wedge ?r(X, Y)$\\
SUCH THAT COUNT($livesIn(x,y)$)$>= k$\\

\noindent with $?r(X, Y) \in \{?r(x,w), ?r(z,w), ?r(w,x), ?r(w,z) \}$, i.e., each possible join combination of a new dangling atom,
where $w$ is an arbitrary fresh variable. For intermediate rules, dangling atoms are joined on the non-closed variables; $z$ and $y$ in this example.
% \comment{Fabian}{$w$ does not appear at all! This has to be rewritten!} \comment{Katja}{Has this been rewritten and the comment remained?}
If the rule is closed, dangling atoms are joined on all the variables appearing in the rule.
%then the system will fire queries for every possible join combination of the new dangling atom with any (all) of the variables appearing in the rule.
%\comment{Katja}{Wouldn't it be nice to have an example here for what ``for every possible join combination'' means in practice, e.g., in relation to the above example with $livesIn$?}
%If the rule is closed, then the new atom is joined with all the variables in the rule.
%\comment{Katja}{How can the rule be closed if the queue in Algorithm 1 contains only non-closed rules? Shouldn't $\mathcal{O_D}$ leave a non-closed variable?}
%\comment{Chris}{In the queue we can have both closed and non closed rules that we want to refine further.}

In the same fashion, when applying the $\mathcal{O_C}$ to our example, the atom $?r(X, Y)$ can take values in $\{ ?r(z,y), ?r(y,z) \}$.
% \\ \\
% SELECT $?r$, COUNT($livesIn(x,y)$)\\
% WHERE $livesIn(x,y) \wedge isMarriedTo(x,z) \wedge\; ?r(z,y)$\\
% SUCH THAT COUNT($livesIn(x,y)$)$>= k$
% \\ \\
%\comment{Katja}{HAVING without GROUP BY, see above}
%is one of the ways to apply the operator $\mathcal{O_C}$.
The method picks all pairs of non-closed variables to close the query pattern. If there are not enough non-closed variables, the operator tries
all possible pairs of variables in the rule.

Finally, the instantiation operator $\mathcal{O_I}$ is implemented in two steps. We first apply the operator $\mathcal{O_D}$ to produce a set of intermediate rules with a new dangling atom and a new fresh variable.
Then for each rule, we fire a count-projection query on the fresh variable. This step provides bindings for one of the arguments of the relation.
For instance, given the following intermediate rule with a dangling atom,

\begin{center}
\emph{citizenOf}$(x,u) \; \wedge$ \emph{isMarriedTo}$(x,z) \Rightarrow $ \emph{livesIn}$(x,y)$
\end{center}

\noindent the system will fire the following query on the fresh variable $?u$
\\ \\
\noindent
SELECT $?u$, COUNT($livesIn(x,y)$) WHERE\\
$livesIn(x,y) \; \wedge \; isMarriedTo(x,z) \; \wedge citizenOf(x, ?u)$\\
SUCH THAT COUNT($livesIn(x,y)$)$>= k$\\

\noindent Each binding of $?u$ forms a new rule that will be enqueued and evaluated for confidence.

In this way, projection queries allow us to choose the relationships and entities for the operators in such a way that the head coverage for the new rules is guaranteed to be above $\theta$.
% \comment{Katja}{Should we add $\theta$ here explicitly?}
% Fabian: Theta was introduced previously, so we should be fine.
Next, we discuss how to implement projection queries efficiently.

\subsection{Implementation} \label{subsec:implementation}

% \comment{Fabian}{I think the following is crucial if we submit to a database journal. We just have to make sure it is correct...} \comment{Luis: }{We should include
% also the nested case!}
\paragraph{SQL and SPARQL} Count-projection queries are essential for the efficiency of our system. Yet, standard database implementations do not provide special support for these types of queries. Assuming that the KB $\mathcal{K}$ is stored as a three-columns table
(i.e., each fact is a row with three elements, ($x_1$, $x_r$, $x_2$)), the count-projection query template in SQL would be:

\indented{SELECT DISTINCT R.$?x$, SUM(R.$N$) FROM (\\
\hspace*{1ex} SELECT $?x$, COUNT(*) AS N \\
\hspace*{1ex} FROM $\mathcal{K}$ AS H, $\mathcal{K}$ AS $B_1$, \dots $\mathcal{K}$ AS $B_n$ \\
\hspace*{1ex} WHERE $H.x_i$ = $B_j.x_m$ AND \dots \\
\hspace*{1ex} GROUP BY $?x$, $H.x_1$, $H.x_r$, $H.x_2$) AS R \\
GROUP BY R.$?x$ \\
HAVING SUM(R.$N$) $>= k$
}

Here, $B_n$ is the new atom added to the query. $?x$ is a placeholder (not a SPARQL variable), with $?x \in \{B_n.x_1, B_n.x_r, B_n.x_2\}$. \comment{Katja}{Why not use a different symbol for $?x$, e.g., $\#x$?}
The expression SUM(R.$N$) calculates the support of the whole query pattern for each value of $?x$.

The WHERE clause contains all join columns and conditions between atom tables.

If we apply this scheme to add a closing atom $?r(z,y)$ to our example rule
\emph{isMarriedTo}$(x,z) \Rightarrow $ \emph{livesIn}$(x,y)$, we would generate the query

\indented{SELECT $R.x_r$, SUM($R.N$) FROM ( \\
 \hspace*{1ex} SELECT $B_2.x_r$ AS $x_r$, COUNT(*) AS N\\
 \hspace*{1ex} FROM $\mathcal{K}$ AS H, $\mathcal{K}$ AS $B_1$, $\mathcal{K}$ AS $B_2$\\
 \hspace*{1ex} WHERE $H.x_1$ = $B_1.x_1$ AND $H.x_2 = B_2.x_2$ \\
 \hspace*{1ex} AND $B_2.x_1 = B_1.x_2$ AND $H.x_r = ``livesIn"$ \\
 \hspace*{1ex} AND $B_1.x_r = ``isMarriedTo"$ \\
 \hspace*{1ex} GROUP BY $B_2.x_r$, $H.x_1$, $H.x_r$, $H.x_2$) AS R\\
 GROUP BY $R.x_r$ \\
 HAVING SUM($R.N$) $>= k$
}

Our experience shows that for a KB of a few million facts, such kind of queries can easily take several minutes on an off-the-shelf RDBMS.
Hence, efficient SPARQL engines such as RDF-3X \cite{rdf3x} are an alternative option. In SPARQL 1.1, the count-projection query template is:
\indented{SELECT $?x$, SUM($?support$) \\
 WHERE \{ \\
 \hspace*{2ex} SELECT $?x$, COUNT(*) AS $?support$ \\
 \hspace*{2ex} WHERE \{ \\
 \hspace*{4ex} $H.x_1$, $H.x_r$, $H.x_2$ . \\
 \hspace*{4ex} $B_1.x_1$, $B_1.x_r$, $B_1.x_2$ . \\
 \hspace*{4ex} \dots \\
 \hspace*{4ex} $B_n.x_1$, $B_n.x_r$, $B_n.x_2$ . \\
 \hspace*{2ex} \} \\
 \hspace*{2ex} GROUP BY $?x, H.x_1$ $H.x_r$ $H.x_2$ \\
 \} \\
 GROUP BY $?x$ HAVING SUM($?support$) $ \ge k$ \\
}
As grouping and aggregate functions have only recently become part of the SPARQL 1.1 standard, many existing SPARQL engines do not yet effciently support the extension. 
%RDF-3X does not support aggregate functions in this way. 
%Thus, we would need extensive postprocessing of query results to compute a projection query.
Hence, we resorted to a custom implementation.

\www{
\paragraph{In-Memory Database} We have implemented a vanilla in-memory database for semantic KBs.
Our implementation indexes the facts aggressively with one index for each permutation of subject, relation, and object.
Each index is a map from the first item to a map from the second item to a set of the third items (e.g., a map from relations to a map from subjects to a set of objects).
This allows retrieving the instantiations of a single atom in constant time.
The existence of a query answer can be checked naively by selecting the atom with fewest instantiations, running through all of its instantiations,
instantiating the remaining atoms accordingly and repeating this process recursively until we find an instantiation of the query that appears in the KB.
Select queries are similar.
}

\www{
\paragraph{Implementation of Projection Queries} Algorithm \ref{algi} shows how we answer projection queries.
The algorithm takes as input a selection variable $?x$, a projection atom $H:=R(X,Y)$, remaining atoms $B_1, ... B_n$, a constant $k$, and a KB $\mathcal{K}$.
We first check whether $?x$ appears in the projection atom.
If that is the case, we run through all instantiations of the projection atom, instantiate the query accordingly, and check for existence.
Each existing instantiation increases the counter for the respective value of $?x$. We return all values whose counter exceeds $k$.
If the selection variable does not appear in the projection atom, we iterate through all instantiations of the projection atom.
We instantiate the query accordingly, and fire a SELECT query for $?x$. We increase the counter for each value of $?x$. We report all values whose counter exceeds $k$.
}


\paragraph{Summary}
The AMIE algorithm iteratively builds more complex rules from simpler rules by the help of three operators.
It takes advantage of monotonic measures to prune the search space efficiently and also takes care of duplicate elimination.
We have identified projection queries as the crucial type of queries for rule mining.
Since standard database systems and standard SPARQL systems provide no specifically tuned support for these queries,
we have implemented a vanilla in-memory database, which has specific support for projection queries.
Our entire implementation is in Java. The code can be downloaded from our Web site\footnote{\url{http://mpi-inf.mpg.de/departments/ontologies/projects/amie}}.

\www{

\begin{algorithm}
\caption{Answering Projection Queries}
\label{algi}
\begin{algorithmic}
\Function{SELECT}{$?x$, $R(X,Y) \wedge B_1 \wedge ... \wedge B_n$, $k$, $\mathcal{K}$}
    \State $map = \emptyset$
    \If {$~~R\equiv\; ?x ~~\vee~~ X\equiv\; ?x ~~\vee~~ Y\equiv\; ?x~~$}
	  \ForAll{instantiations $r(x,y)$ of $R(X,Y) \in \mathcal{K}$}
	    \State $q=B_1 \wedge ... \wedge B_n$
		\State In $q$, replace $R$ by $r$, $X$ by $x$, $Y$ by $y$
	    \If{exists instantiation $q \in \mathcal{K}$}
		\State $map($value of $?x)++$
		\EndIf
	  \EndFor
	\Else
	  \ForAll{instantiations $r(x,y)$ of $R(X,Y) \in \mathcal{K}$}
	    \State $q=B_1 \wedge ... \wedge B_n$
		\State In $q$, replace $R$ by $r$, $X$ by $x$, $Y$ by $y$
	    \ForAll{$x\in${\small{}SELECT $?x$ FROM $\mathcal{K}$ WHERE $q$}}
		  \State $map(x)++$
		\EndFor
	  \EndFor
	\EndIf
	\State \Return $\{ x : map(x)\geq k \}$
\EndFunction
\end{algorithmic}
\end{algorithm}
\ \\[-1cm]
}